#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–è¿è¡ŒCoInceptionæ¨¡å‹çš„è®­ç»ƒã€åˆ†æå’Œå¯è§†åŒ–æµç¨‹

Usage:
    # å•æ•°æ®é›†åˆ†æ
    python run_complete_analysis.py <dataset_name> <run_name> --loader <loader> [options]
    
    # å…¨éƒ¨æ•°æ®é›†éå†åˆ†æ
    python run_complete_analysis.py --all-datasets

Example:
    # å•æ•°æ®é›†åˆ†æ
    python run_complete_analysis.py Chinatown UCR --loader UCR --batch-size 8 --repr-dims 320 --gpu 0
    
    # å…¨éƒ¨æ•°æ®é›†éå†åˆ†æ
    python run_complete_analysis.py --all-datasets
"""

import os
import sys
import argparse
import subprocess
import shutil
import time
import pickle
import numpy as np
from tqdm import tqdm

def check_dataset_files(dataset_name, loader):
    """æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    if loader == 'UCR':
        data_dir = os.path.join(os.getcwd(), 'data', 'UCR')
        train_file = os.path.join(data_dir, f'{dataset_name}', f'{dataset_name}_TRAIN.ts')
        test_file = os.path.join(data_dir, f'{dataset_name}', f'{dataset_name}_TEST.ts')
        return os.path.exists(train_file) and os.path.exists(test_file)
    elif loader == 'UEA':
        data_dir = os.path.join(os.getcwd(), 'data', 'UEA')
        train_file = os.path.join(data_dir, f'{dataset_name}', f'{dataset_name}_TRAIN.arff')
        test_file = os.path.join(data_dir, f'{dataset_name}', f'{dataset_name}_TEST.arff')
        return os.path.exists(train_file) and os.path.exists(test_file)
    else:
        # å…¶ä»–ç±»å‹çš„æ•°æ®é›†æ£€æŸ¥
        return True  # æš‚æ—¶è·³è¿‡å…¶ä»–ç±»å‹çš„æ•°æ®é›†æ£€æŸ¥

def download_datasets_guide():
    """æ˜¾ç¤ºæ•°æ®é›†ä¸‹è½½æŒ‡å—"""
    guide = """
    === æ•°æ®é›†æ–‡ä»¶ç¼ºå¤±é”™è¯¯ ===
    
    å½“å‰ç³»ç»Ÿç¼ºå°‘å¿…è¦çš„æ•°æ®é›†æ–‡ä»¶ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è§£å†³ï¼š
    
    1. UEAæ•°æ®é›†ä¸‹è½½:
       è®¿é—®: http://www.timeseriesclassification.com/dataset.php
       ä¸‹è½½: UEA & UCR Time Series Classification Repository
       è§£å‹åé‡å‘½åä¸º'UEA'æ–‡ä»¶å¤¹
       ç§»åŠ¨åˆ°: /home/codeserver/CoInception/data/UEA/
    
    2. UCRæ•°æ®é›†ä¸‹è½½:
       è®¿é—®: https://www.cs.ucr.edu/~eamonn/time_series_data_2018
       ä¸‹è½½UCRæ—¶é—´åºåˆ—æ•°æ®é›†
       è§£å‹åé‡å‘½åä¸º'UCR'æ–‡ä»¶å¤¹  
       ç§»åŠ¨åˆ°: /home/codeserver/CoInception/data/UCR/
    
    3. å…¶ä»–æ•°æ®é›†è¯·å‚è€ƒé¡¹ç›®README.mdæ–‡ä»¶
    
    è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹: /home/codeserver/CoInception/data/download_uea_data.sh
    
    è¿è¡Œä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹å¸®åŠ©:
    cat /home/codeserver/CoInception/data/download_uea_data.sh
    """
    print(guide)

class CompleteAnalysisRunner:
    def __init__(self, args):
        self.args = args
        self.base_dir = os.getcwd()
        self.training_dir = os.path.join(self.base_dir, 'training')
        self.results_dir = os.path.join(self.base_dir, 'results')
        
        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        os.makedirs(self.results_dir, exist_ok=True)
        
        # é¢„è®¾å‚æ•°é…ç½®
        self.preset_params = {
            'UCR': {
                'batch_size': 8,
                'repr_dims': 320,
                'max_threads': 8,
                'seed': 42
            },
            'UEA': {
                'batch_size': 8,
                'repr_dims': 320,
                'max_threads': 8,
                'seed': 42
            },
            'forecast_csv': {
                'batch_size': 8,
                'repr_dims': 320,
                'max_threads': 8,
                'seed': 42
            },
            'forecast_csv_univar': {
                'batch_size': 8,
                'repr_dims': 320,
                'max_threads': 8,
                'seed': 42
            },
            'anomaly': {
                'batch_size': 8,
                'repr_dims': 320,
                'max_threads': 8,
                'seed': 42
            },
            'anomaly_coldstart': {
                'batch_size': 8,
                'repr_dims': 320,
                'max_threads': 8,
                'seed': 42
            }
        }
        
        # æ•°æ®é›†é…ç½®
        self.dataset_configs = {
            'UCR': {
                'loader': 'UCR',
                'datasets': self._get_ucr_datasets()
            },
            'UEA': {
                'loader': 'UEA',
                'datasets': self._get_uea_datasets()
            },
            'ETT': {
                'loader': 'forecast_csv',
                'datasets': ['ETTh1', 'ETTh2', 'ETTm1']
            },
            'Electricity': {
                'loader': 'forecast_csv',
                'datasets': ['electricity']
            },
            'Yahoo': {
                'loader': 'anomaly',
                'datasets': ['yahoo']
            },
            'KPI': {
                'loader': 'anomaly',
                'datasets': ['kpi']
            }
        }
    
    def run_training(self):
        """è¿è¡Œè®­ç»ƒè„šæœ¬"""
        print("=" * 60)
        print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        print("=" * 60)
        
        # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œè€Œä¸æ˜¯å½“å‰å·¥ä½œç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # æ„å»ºè®­ç»ƒå‘½ä»¤
        train_cmd = [
            sys.executable, 'train.py',
            self.args.dataset_name,
            self.args.run_name,
            '--loader', self.args.loader,
            '--batch-size', str(self.args.batch_size),
            '--repr-dims', str(self.args.repr_dims),
            '--gpu', str(self.args.gpu),
            '--eval'
        ]
        
        # æ·»åŠ å¯é€‰å‚æ•°
        if self.args.max_threads:
            train_cmd.extend(['--max-threads', str(self.args.max_threads)])
        if self.args.seed:
            train_cmd.extend(['--seed', str(self.args.seed)])
        if self.args.save_ckpt:
            train_cmd.append('--save_ckpt')
        if self.args.irregular > 0:
            train_cmd.extend(['--irregular', str(self.args.irregular)])
        
        print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(train_cmd)}")
        
        # è¿è¡Œè®­ç»ƒå‘½ä»¤ï¼Œä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•ä½œä¸ºå·¥ä½œç›®å½•
        result = subprocess.run(train_cmd, cwd=script_dir, capture_output=True, text=True)
        
        if result.returncode != 0:
            print(f"è®­ç»ƒå¤±è´¥! é”™è¯¯ä¿¡æ¯:")
            print(result.stderr)
            sys.exit(1)
        
        print(result.stdout)
        print("=" * 60)
        print("è®­ç»ƒå®Œæˆ!")
        print("=" * 60)
        
        # æå–ä¸­é—´æ•°æ®è·¯å¾„
        self.intermediate_data_path = self._find_intermediate_data_path()
        print(f"ä¸­é—´æ•°æ®ä¿å­˜è·¯å¾„: {self.intermediate_data_path}")
    
    def _find_intermediate_data_path(self):
        """æŸ¥æ‰¾è®­ç»ƒç”Ÿæˆçš„ä¸­é—´æ•°æ®è·¯å¾„"""
        # è®­ç»ƒç›®å½•æ ¼å¼: training/<dataset_name>__<run_name>/
        training_run_dir = f"{self.args.dataset_name}__{self.args.run_name}"
        
        # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œè€Œä¸æ˜¯å½“å‰å·¥ä½œç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        intermediate_data_path = os.path.join(script_dir, 'training', training_run_dir, 'intermediate_data.pkl')
        
        if not os.path.exists(intermediate_data_path):
            print(f"æ‰¾ä¸åˆ°ä¸­é—´æ•°æ®æ–‡ä»¶: {intermediate_data_path}")
            sys.exit(1)
        
        return intermediate_data_path
    
    @staticmethod
    def check_dataset_files(dataset_name, loader):
        """æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # UEAæ•°æ®é›†æ–‡ä»¶è·¯å¾„
        if loader == 'UEA':
            uea_dir = os.path.join(script_dir, 'data', 'UEA', dataset_name)
            train_file = os.path.join(uea_dir, f"{dataset_name}_TRAIN.ts")
            test_file = os.path.join(uea_dir, f"{dataset_name}_TEST.ts")
            
            if not os.path.exists(train_file) or not os.path.exists(test_file):
                print(f"âŒ UEAæ•°æ®é›†æ–‡ä»¶ç¼ºå¤±:")
                print(f"   ç¼ºå°‘: {train_file}")
                print(f"   ç¼ºå°‘: {test_file}")
                print(f"   ç›®å½•å†…å®¹: {os.listdir(uea_dir) if os.path.exists(uea_dir) else 'ç›®å½•ä¸å­˜åœ¨'}")
                return False
            print(f"âœ… UEAæ•°æ®é›†æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
            return True
            
        # UCRæ•°æ®é›†æ–‡ä»¶è·¯å¾„
        elif loader == 'UCR':
            ucr_dir = os.path.join(script_dir, 'data', 'UCR', dataset_name)
            train_file = os.path.join(ucr_dir, f"{dataset_name}_TRAIN.ts")
            test_file = os.path.join(ucr_dir, f"{dataset_name}_TEST.ts")
            
            if not os.path.exists(train_file) or not os.path.exists(test_file):
                print(f"âŒ UCRæ•°æ®é›†æ–‡ä»¶ç¼ºå¤±:")
                print(f"   ç¼ºå°‘: {train_file}")
                print(f"   ç¼ºå°‘: {test_file}")
                return False
            print(f"âœ… UCRæ•°æ®é›†æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
            return True
            
        # ETTé¢„æµ‹æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        elif loader == 'forecast_csv':
            if dataset_name.startswith('ETT'):
                ett_file = os.path.join(script_dir, 'data', 'ETT', f"{dataset_name}.csv")
                if not os.path.exists(ett_file):
                    print(f"âŒ ETTæ•°æ®é›†æ–‡ä»¶ç¼ºå¤±:")
                    print(f"   ç¼ºå°‘: {ett_file}")
                    print(f"   ETTç›®å½•å†…å®¹: {os.listdir(os.path.join(script_dir, 'data', 'ETT')) if os.path.exists(os.path.join(script_dir, 'data', 'ETT')) else 'ETTç›®å½•ä¸å­˜åœ¨'}")
                    return False
                print(f"âœ… ETTæ•°æ®é›†æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
                return True
            else:
                # å…¶ä»–CSVæ–‡ä»¶æ£€æŸ¥
                csv_file = os.path.join(script_dir, 'data', f"{dataset_name}.csv")
                if not os.path.exists(csv_file):
                    print(f"âŒ CSVæ•°æ®é›†æ–‡ä»¶ç¼ºå¤±:")
                    print(f"   ç¼ºå°‘: {csv_file}")
                    return False
                print(f"âœ… CSVæ•°æ®é›†æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
                return True
                
        # å¼‚å¸¸æ£€æµ‹æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        elif loader == 'anomaly':
            pkl_file = os.path.join(script_dir, 'data', f"{dataset_name}.pkl")
            if not os.path.exists(pkl_file):
                print(f"âŒ å¼‚å¸¸æ£€æµ‹æ•°æ®é›†æ–‡ä»¶ç¼ºå¤±:")
                print(f"   ç¼ºå°‘: {pkl_file}")
                return False
            print(f"âœ… å¼‚å¸¸æ£€æµ‹æ•°æ®é›†æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
            return True
            
        else:
            print(f"âš ï¸ æœªçŸ¥çš„æ•°æ®é›†ç±»å‹: {loader}")
            return True  # æš‚æ—¶è·³è¿‡æ£€æŸ¥
    
    @staticmethod
    def download_datasets_guide():
        """æä¾›æ•°æ®é›†ä¸‹è½½æŒ‡å—"""
        print("\n" + "=" * 80)
        print("æ•°æ®é›†ä¸‹è½½å’Œè®¾ç½®æŒ‡å—")
        print("=" * 80)
        
        print("\nğŸ“ UEA æ•°æ®é›†:")
        print("-" * 30)
        print("1. è®¿é—®å®˜ç½‘: http://www.timeseriesclassification.com/")
        print("2. è¿›å…¥ 'Datasets' é¡µé¢")
        print("3. ä¸‹è½½æ‰€éœ€çš„å¤šå˜é‡æ—¶é—´åºåˆ—æ•°æ®é›†")
        print("4. è§£å‹æ–‡ä»¶å¹¶å°†å…¶æ”¾åœ¨ data/UEA/ ç›®å½•ä¸‹")
        print("5. ç¡®ä¿æ–‡ä»¶å‘½åä¸º: {æ•°æ®é›†å}_TRAIN.arff å’Œ {æ•°æ®é›†å}_TEST.arff")
        print("\nç¤ºä¾‹:")
        print("   - data/UEA/BasicMotions_TRAIN.arff")
        print("   - data/UEA/BasicMotions_TEST.arff")
        
        print("\nğŸ“ UCR æ•°æ®é›†:")
        print("-" * 30)
        print("1. è®¿é—®å®˜ç½‘: http://www.timeseriesclassification.com/")
        print("2. è¿›å…¥ 'UCR Archive' é¡µé¢")
        print("3. ä¸‹è½½æ‰€éœ€çš„å•å˜é‡æ—¶é—´åºåˆ—æ•°æ®é›†")
        print("4. è§£å‹æ–‡ä»¶å¹¶å°†å…¶æ”¾åœ¨ data/UCR/ ç›®å½•ä¸‹")
        print("5. ç¡®ä¿æ–‡ä»¶å‘½åä¸º: {æ•°æ®é›†å}_TRAIN.ts å’Œ {æ•°æ®é›†å}_TEST.ts")
        print("\nç¤ºä¾‹:")
        print("   - data/UCR/Chinatown_TRAIN.ts")
        print("   - data/UCR/Chinatown_TEST.ts")
        
        print("\nğŸ“ æ•°æ®ä¸‹è½½è„šæœ¬:")
        print("-" * 30)
        print("è¿è¡Œä»¥ä¸‹å‘½ä»¤æ‰§è¡Œè‡ªåŠ¨ä¸‹è½½è„šæœ¬:")
        print("   bash data/download_uea_data.sh")
        
        print("\n" + "=" * 80)
        print("è®¾ç½®å®Œæˆåï¼Œé‡æ–°è¿è¡Œè®­ç»ƒå‘½ä»¤")
        print("=" * 80)
    
    def run_analysis(self):
        """è¿è¡Œåˆ†æè„šæœ¬"""
        print("\n" + "=" * 60)
        print("å¼€å§‹åˆ†æä¸­é—´æ•°æ®...")
        print("=" * 60)
        
        # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œè€Œä¸æ˜¯å½“å‰å·¥ä½œç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # æ„å»ºåˆ†ææŠ¥å‘Šç›®å½•
        analysis_report_dir = os.path.join(self.results_dir, f"{self.args.dataset_name}__{self.args.run_name}_analysis")
        
        # æ„å»ºåˆ†æå‘½ä»¤
        analysis_cmd = [
            sys.executable, 'analyze_robustness.py',
            self.intermediate_data_path,
            '--report_dir', analysis_report_dir
        ]
        
        print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(analysis_cmd)}")
        
        # è¿è¡Œåˆ†æå‘½ä»¤ï¼Œä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•ä½œä¸ºå·¥ä½œç›®å½•
        result = subprocess.run(analysis_cmd, cwd=script_dir, capture_output=True, text=True)
        
        if result.returncode != 0:
            print(f"åˆ†æå¤±è´¥! é”™è¯¯ä¿¡æ¯:")
            print(result.stderr)
            sys.exit(1)
        
        print(result.stdout)
        print("=" * 60)
        print("åˆ†æå®Œæˆ!")
        print("=" * 60)
        
        self.analysis_report_dir = analysis_report_dir
    
    def run_visualization(self):
        """è¿è¡Œå¯è§†åŒ–è„šæœ¬"""
        print("\n" + "=" * 60)
        print("å¼€å§‹ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š...")
        print("=" * 60)
        
        # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œè€Œä¸æ˜¯å½“å‰å·¥ä½œç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # æ„å»ºå¯è§†åŒ–æŠ¥å‘Šç›®å½•
        visualization_report_dir = os.path.join(self.results_dir, f"{self.args.dataset_name}__{self.args.run_name}_visualization")
        
        # æ„å»ºå¯è§†åŒ–å‘½ä»¤
        visualization_cmd = [
            sys.executable, 'visualize_robustness.py',
            self.intermediate_data_path,
            '--report_dir', visualization_report_dir
        ]
        
        print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(visualization_cmd)}")
        
        # è¿è¡Œå¯è§†åŒ–å‘½ä»¤ï¼Œä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•ä½œä¸ºå·¥ä½œç›®å½•
        result = subprocess.run(visualization_cmd, cwd=script_dir, capture_output=True, text=True)
        
        if result.returncode != 0:
            print(f"å¯è§†åŒ–å¤±è´¥! é”™è¯¯ä¿¡æ¯:")
            print(result.stderr)
            sys.exit(1)
        
        print(result.stdout)
        print("=" * 60)
        print("å¯è§†åŒ–å®Œæˆ!")
        print("=" * 60)
        
        self.visualization_report_dir = visualization_report_dir
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ç”Ÿæˆæ€»ç»“æŠ¥å‘Š...")
        print("=" * 60)
        
        summary_path = os.path.join(self.results_dir, f"{self.args.dataset_name}__{self.args.run_name}_summary.txt")
        
        with open(summary_path, 'w') as f:
            f.write("CoInception å®Œæ•´åˆ†ææ€»ç»“\n")
            f.write("=" * 50 + "\n\n")
            
            f.write("1. è®­ç»ƒé…ç½®\n")
            f.write("-" * 30 + "\n")
            f.write(f"æ•°æ®é›†åç§°: {self.args.dataset_name}\n")
            f.write(f"è¿è¡Œåç§°: {self.args.run_name}\n")
            f.write(f"åŠ è½½å™¨ç±»å‹: {self.args.loader}\n")
            f.write(f"æ‰¹å¤§å°: {self.args.batch_size}\n")
            f.write(f"è¡¨ç¤ºç»´åº¦: {self.args.repr_dims}\n")
            f.write(f"GPUç¼–å·: {self.args.gpu}\n")
            if self.args.max_threads:
                f.write(f"æœ€å¤§çº¿ç¨‹æ•°: {self.args.max_threads}\n")
            if self.args.seed:
                f.write(f"éšæœºç§å­: {self.args.seed}\n")
            f.write(f"æ˜¯å¦ä¿å­˜æ¨¡å‹: {self.args.save_ckpt}\n")
            if self.args.irregular > 0:
                f.write(f"ç¼ºå¤±æ•°æ®æ¯”ä¾‹: {self.args.irregular}\n")
            f.write("\n")
            
            f.write("2. ç»“æœæ–‡ä»¶\n")
            f.write("-" * 30 + "\n")
            f.write(f"ä¸­é—´æ•°æ®è·¯å¾„: {self.intermediate_data_path}\n")
            f.write(f"åˆ†ææŠ¥å‘Šç›®å½•: {self.analysis_report_dir}\n")
            f.write(f"å¯è§†åŒ–æŠ¥å‘Šç›®å½•: {self.visualization_report_dir}\n")
            f.write("\n")
            
            f.write("3. åˆ†æç»“æœ\n")
            f.write("-" * 30 + "\n")
            f.write("è¯¦ç»†åˆ†æç»“æœè¯·æŸ¥çœ‹åˆ†ææŠ¥å‘Šç›®å½•ä¸­çš„æ–‡ä»¶\n")
            f.write("\n")
            
            f.write("4. å¯è§†åŒ–ç»“æœ\n")
            f.write("-" * 30 + "\n")
            f.write("è¯¦ç»†å¯è§†åŒ–ç»“æœè¯·æŸ¥çœ‹å¯è§†åŒ–æŠ¥å‘Šç›®å½•ä¸­çš„HTMLæ–‡ä»¶\n")
        
        print(f"æ€»ç»“æŠ¥å‘Šç”Ÿæˆè·¯å¾„: {summary_path}")
        print("=" * 60)
        print("æ‰€æœ‰åˆ†ææµç¨‹å®Œæˆ!")
        print("=" * 60)
    
    def _get_ucr_datasets(self):
        """è·å–UCRæ•°æ®é›†åˆ—è¡¨"""
        # é»˜è®¤UCRæ•°æ®é›†åˆ—è¡¨
        return ['Chinatown', 'ItalyPowerDemand', 'TwoLeadECG', 'ECGFiveDays', 'GunPoint']
    
    def _get_ucr_datasets_from_script(self):
        """ä»ucr.shè„šæœ¬ä¸­æå–UCRæ•°æ®é›†åˆ—è¡¨ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        ucr_script_path = os.path.join(self.base_dir, 'scripts', 'ucr.sh')
        if os.path.exists(ucr_script_path):
            with open(ucr_script_path, 'r') as f:
                content = f.read()
            # æå–æ•°æ®é›†åç§°
            datasets = []
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ•°æ®é›†åç§°
            import re
            matches = re.findall(r'python -u train.py\s+([\w-]+)\s+UCR', content)
            return matches[:10]  # è¿”å›å‰10ä¸ªæ•°æ®é›†ä½œä¸ºç¤ºä¾‹
        else:
            return []
    
    def _get_uea_datasets(self):
        """è·å–UEAæ•°æ®é›†åˆ—è¡¨"""
        # é»˜è®¤UEAæ•°æ®é›†åˆ—è¡¨
        return ['BasicMotions', 'FaceDetection', 'Heartbeat', 'UWaveGestureLibraryAll', 'Libras']
    
    def run_single_dataset(self, dataset_name, loader, run_name=None, **kwargs):
        """è¿è¡Œå•ä¸ªæ•°æ®é›†çš„åˆ†ææµç¨‹"""
        # ä½¿ç”¨æ•°æ®é›†åç§°ä½œä¸ºé»˜è®¤è¿è¡Œåç§°
        if run_name is None:
            run_name = loader
        
        # æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not check_dataset_files(dataset_name, loader):
            print(f"âŒ æ•°æ®é›†æ–‡ä»¶ç¼ºå¤±: {dataset_name}")
            print(f"è¯·æ£€æŸ¥ {loader} æ•°æ®é›†æ˜¯å¦å·²æ­£ç¡®ä¸‹è½½åˆ° data/{loader} ç›®å½•")
            download_datasets_guide()
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_name} ({loader})")
        
        # è·å–é¢„è®¾å‚æ•°
        preset_params = self.preset_params.get(loader, {})
        
        # åˆå¹¶å‚æ•°ï¼škwargs > å‘½ä»¤è¡Œå‚æ•° > é¢„è®¾å‚æ•°
        params = {
            **preset_params,
            **vars(self.args),
            **kwargs
        }
        
        # æ›´æ–°argså¯¹è±¡
        for key, value in params.items():
            if hasattr(self.args, key):
                setattr(self.args, key, value)
        
        self.args.dataset_name = dataset_name
        self.args.run_name = run_name
        self.args.loader = loader
        
        print(f"\n{'-'*80}")
        print(f"å¼€å§‹åˆ†ææ•°æ®é›†: {dataset_name}")
        print(f"åŠ è½½å™¨ç±»å‹: {loader}")
        print(f"è¿è¡Œåç§°: {run_name}")
        print(f"{'-'*80}")
        
        # è¿è¡Œè®­ç»ƒã€åˆ†æå’Œå¯è§†åŒ–
        self.run_training()
        self.run_analysis()
        self.run_visualization()
        
        # ä¿å­˜å½“å‰ç»“æœè·¯å¾„ï¼Œç”¨äºç”Ÿæˆæœ€ç»ˆæ€»ç»“
        result_info = {
            'dataset_name': dataset_name,
            'loader': loader,
            'run_name': run_name,
            'intermediate_data_path': self.intermediate_data_path,
            'analysis_report_dir': self.analysis_report_dir,
            'visualization_report_dir': self.visualization_report_dir
        }
        
        return result_info
    
    def run_all_datasets(self):
        """è¿è¡Œæ‰€æœ‰æ•°æ®é›†çš„åˆ†ææµç¨‹"""
        print("=" * 80)
        print("å¼€å§‹éå†æ‰€æœ‰æ•°æ®é›†")
        print("=" * 80)
        
        all_results = []
        total_datasets = sum(len(config['datasets']) for config in self.dataset_configs.values())
        current_dataset = 0
        
        start_time = time.time()
        
        for category, config in self.dataset_configs.items():
            loader = config['loader']
            datasets = config['datasets']
            
            print(f"\n{'-'*80}")
            print(f"å¤„ç†æ•°æ®é›†ç±»åˆ«: {category}")
            print(f"åŠ è½½å™¨ç±»å‹: {loader}")
            print(f"æ•°æ®é›†æ•°é‡: {len(datasets)}")
            print(f"{'-'*80}")
            
            for dataset in tqdm(datasets, desc=f"{category} æ•°æ®é›†", leave=True):
                current_dataset += 1
                
                try:
                    result_info = self.run_single_dataset(dataset, loader)
                    all_results.append(result_info)
                    
                    # ä¿å­˜å½“å‰è¿›åº¦
                    progress_path = os.path.join(self.results_dir, 'analysis_progress.pkl')
                    with open(progress_path, 'wb') as f:
                        pickle.dump(all_results, f)
                    
                except KeyboardInterrupt:
                    print("\nåˆ†ææµç¨‹è¢«ç”¨æˆ·ä¸­æ–­!")
                    # ä¿å­˜å½“å‰è¿›åº¦
                    progress_path = os.path.join(self.results_dir, 'analysis_progress.pkl')
                    with open(progress_path, 'wb') as f:
                        pickle.dump(all_results, f)
                    # ç”Ÿæˆæœ€ç»ˆæ€»ç»“æŠ¥å‘Š
                    end_time = time.time()
                    total_time = end_time - start_time
                    self.generate_final_summary(all_results, total_time)
                    print(f"\nå·²å¤„ç†æ•°æ®é›†æ•°é‡: {len(all_results)}/{total_datasets}")
                    print(f"æ€»è€—æ—¶: {time.strftime('%H:%M:%S', time.gmtime(total_time))}")
                    sys.exit(0)
                    
                except Exception as e:
                    print(f"âŒ å¤„ç†æ•°æ®é›† {dataset} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    print(f"ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ•°æ®é›†...")
                    continue
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # ç”Ÿæˆæœ€ç»ˆæ€»ç»“æŠ¥å‘Š
        self.generate_final_summary(all_results, total_time)
        
        print("\n" + "=" * 80)
        print("æ‰€æœ‰æ•°æ®é›†åˆ†æå®Œæˆ!")
        print(f"æ€»è€—æ—¶: {time.strftime('%H:%M:%S', time.gmtime(total_time))}")
        print(f"æˆåŠŸåˆ†ææ•°æ®é›†æ•°é‡: {len(all_results)}/{total_datasets}")
        print("=" * 80)
    
    def generate_final_summary(self, all_results, total_time):
        """ç”Ÿæˆæ‰€æœ‰æ•°æ®é›†çš„æœ€ç»ˆæ€»ç»“æŠ¥å‘Š"""
        summary_path = os.path.join(self.results_dir, 'final_summary.txt')
        
        with open(summary_path, 'w') as f:
            f.write("CoInception æ‰€æœ‰æ•°æ®é›†åˆ†ææ€»ç»“\n")
            f.write("=" * 60 + "\n\n")
            
            f.write("1. åˆ†ææ¦‚è§ˆ\n")
            f.write("-" * 30 + "\n")
            f.write(f"æ€»æ•°æ®é›†æ•°é‡: {len(all_results)}\n")
            f.write(f"æ€»è€—æ—¶: {time.strftime('%H:%M:%S', time.gmtime(total_time))}\n")
            f.write(f"å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time() - total_time))}\n")
            f.write(f"ç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\n")
            f.write("\n")
            
            f.write("2. æ•°æ®é›†åˆ†æç»“æœ\n")
            f.write("-" * 30 + "\n")
            
            # æŒ‰æ•°æ®é›†ç±»åˆ«åˆ†ç»„
            category_results = {}
            for result in all_results:
                category = result['loader']
                if category not in category_results:
                    category_results[category] = []
                category_results[category].append(result)
            
            for category, results in category_results.items():
                f.write(f"\n{category} æ•°æ®é›† ({len(results)}):\n")
                f.write("-" * 20 + "\n")
                
                for result in results:
                    f.write(f"  - æ•°æ®é›†: {result['dataset_name']}\n")
                    f.write(f"     åˆ†ææŠ¥å‘Š: {result['analysis_report_dir']}\n")
                    f.write(f"     å¯è§†åŒ–æŠ¥å‘Š: {result['visualization_report_dir']}\n")
            
            f.write("\n3. ç»“æœæ–‡ä»¶\n")
            f.write("-" * 30 + "\n")
            f.write(f"åˆ†æè¿›åº¦æ–‡ä»¶: {os.path.join(self.results_dir, 'analysis_progress.pkl')}\n")
            f.write(f"æœ€ç»ˆæ€»ç»“æ–‡ä»¶: {summary_path}\n")
            f.write("\n")
            
            f.write("4. ä½¿ç”¨è¯´æ˜\n")
            f.write("-" * 30 + "\n")
            f.write("è¯¦ç»†åˆ†æç»“æœè¯·æŸ¥çœ‹å„æ•°æ®é›†å¯¹åº”çš„åˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–æŠ¥å‘Šç›®å½•\n")
            f.write("å¯è§†åŒ–æŠ¥å‘ŠåŒ…å«HTMLæ–‡ä»¶ï¼Œå¯ç›´æ¥åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æŸ¥çœ‹\n")
        
        print(f"\næœ€ç»ˆæ€»ç»“æŠ¥å‘Šç”Ÿæˆè·¯å¾„: {summary_path}")
    
    def run(self):
        """è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹"""
        try:
            if hasattr(self.args, 'all_datasets') and self.args.all_datasets:
                # è¿è¡Œæ‰€æœ‰æ•°æ®é›†
                self.run_all_datasets()
            else:
                # è¿è¡Œå•ä¸ªæ•°æ®é›†
                self.run_training()
                self.run_analysis()
                self.run_visualization()
                self.generate_summary_report()
        except KeyboardInterrupt:
            print("\nåˆ†ææµç¨‹è¢«ç”¨æˆ·ä¸­æ–­!")
            sys.exit(1)
        except Exception as e:
            print(f"åˆ†ææµç¨‹å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            import traceback
            traceback.print_exc()
            sys.exit(1)

def main():
    parser = argparse.ArgumentParser(description='è‡ªåŠ¨åŒ–è¿è¡ŒCoInceptionæ¨¡å‹çš„è®­ç»ƒã€åˆ†æå’Œå¯è§†åŒ–æµç¨‹')
    
    # æ·»åŠ å…¨éƒ¨æ•°æ®é›†åˆ†æå‚æ•°
    parser.add_argument('--all-datasets', action='store_true', help='éå†æ‰€æœ‰æ•°æ®é›†è¿›è¡Œåˆ†æ')
    
    # å•æ•°æ®é›†åˆ†æå‚æ•°ï¼ˆå½“--all-datasetsæœªæŒ‡å®šæ—¶å¿…éœ€ï¼‰
    parser.add_argument('dataset_name', type=str, nargs='?', help='æ•°æ®é›†åç§°')
    parser.add_argument('run_name', type=str, nargs='?', help='è¿è¡Œåç§°')
    parser.add_argument('--loader', type=str, help='æ•°æ®åŠ è½½å™¨ç±»å‹')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--batch-size', type=int, default=8, help='æ‰¹å¤§å° (é»˜è®¤: 8)')
    parser.add_argument('--repr-dims', type=int, default=320, help='è¡¨ç¤ºç»´åº¦ (é»˜è®¤: 320)')
    parser.add_argument('--gpu', type=int, default=0, help='GPUç¼–å· (é»˜è®¤: 0)')
    parser.add_argument('--max-threads', type=int, default=None, help='æœ€å¤§çº¿ç¨‹æ•°')
    parser.add_argument('--seed', type=int, default=None, help='éšæœºç§å­')
    parser.add_argument('--save_ckpt', action='store_true', help='æ˜¯å¦ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹')
    parser.add_argument('--irregular', type=float, default=0, help='ç¼ºå¤±æ•°æ®æ¯”ä¾‹ (é»˜è®¤: 0)')
    
    args = parser.parse_args()
    
    # å‚æ•°éªŒè¯
    if not args.all_datasets:
        if not args.dataset_name or not args.run_name or not args.loader:
            parser.error('å½“æœªæŒ‡å®š --all-datasets æ—¶ï¼Œå¿…é¡»æä¾› dataset_name, run_name å’Œ --loader å‚æ•°')
    
    runner = CompleteAnalysisRunner(args)
    runner.run()

if __name__ == '__main__':
    main()